\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{xcolor}
\modulolinenumbers[5]

\journal{Journal of Information Processing and Management }

\bibliographystyle{elsarticle-num}

\begin{document}

\begin{frontmatter}

\title{Investigating Facebook's actions against accounts that repeatedly share misinformation}

\author[mymainaddress]{Héloïse Théro\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding authors.}
\ead{thero.heloise@gmail.com}

\author[mymainaddress]{Emmanuel M. Vincent\corref{mycorrespondingauthor}}
\ead{emmanuel.vincent@sciencespo.fr}

\address[mymainaddress]{médialab - Sciences Po, Paris, France}

\begin{abstract}

{\color{red} SHORTEN AND ADD NEW CONDOR RESULTS

Like many web platforms, Facebook is under pressure to regulate misinformation. 
According to the company, users that repeatedly share misinformation (`repeat offenders') will have their distribution reduced, but little is known about the implementation or the efficiency of this measure.
First, combining data from a fact-checking organization and CrowdTangle, we identified a set of public accounts (groups and pages) that have shared misinformation repeatedly. 
While we observe a decrease in engagement for pages (median of $-43\%$) after they shared two or more `false news', such a reduction is not observed for groups. 
However, we discover that groups have been affected in a different way with a sudden drop in their average engagement per post that occurred around June 9, 2020.
No public information was given by Facebook about this sudden decrease.
This drop has cut the groups’ engagement per post in half, but it was compensated by the fact that the overall activity of `repeat offenders' has doubled between 2019 and 2020.
Second, we identified pages that have been warned by Facebook and have shared a screenshot of the `reduced distribution' notification they have received. 
We found that their engagement per post following the notification decreased by a modest amount (median of $-24\%$), with some popular pages actually gaining more engagement.
Our results highlight easy steps Facebook could take to reduce misinformation, such as to enforce their `repeat offenders' policy more forcefully on pages, and to start applying it to groups.}
\end{abstract}

\begin{keyword}
Misinformation\sep Content moderation\sep Algorithmic transparency\sep Facebook\sep Fact-checking\sep Social media analysis
%\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}

This is how you add a footnote.\footnote{This is the footnote message.}

\paragraph{Research questions}
\begin{itemize}
\item Was the policy aiming to reduce the distribution of misinformation repeat offenders actually enforced by Facebook during the 2019-2020 period?
\item If so, what was the magnitude of the reduction applied? And is there a difference between Facebook groups and Facebook pages?
\item Does this action have an impact on the spread of misinformation on Facebook, i.e., can we see a global increase or decrease in engagement for the repeat offender accounts through time?
\end{itemize}

\section{Investigating the reduce policy on accounts repeatedly sharing misinformation (Condor data)}

\subsection{Methods}

We used data from the Social Science One organization \cite{king2020new}, that builds partnerships between academia and private industries such as Facebook to share data and expertise. 
In July 2021, we had access to a new version of the Condor dataset \cite{messing2020facebook}, which contains all URLs shared publicly by at least 100 Facebook users between 2017 and 2021, as well as their fact-checking metadata. 
From this list, we extracted the 6,811 URLs that were shared in 2019 and 2020, that were fact-checked as `False' and whose country in which it was shared most frequently was either the USA, Canada, Great Britain or Australia.

We then replicated as closely as possible the methods used in the previous section. 
Using CrowdTangle, we thus collected all the posts that shared one of the false links between January 1, 2019 and December 31, 2020, and focused on the 706 Facebook accounts (671 Facebook groups and 35 Facebook pages) that spread at least 24 false links. 
Then we used CrowdTangle again to collect all the posts published by those accounts in 2019 and 2020. 
Because the Condor dataset contained the date of the first fact-check done on a URL, we were able to infer the `repeat offender' periods for each account and therefore conduct the same analysis as in the previous section.

Science Feedback being a third-party fact-checker working with Facebook, most of the URLs from Science Feedback were also contained in the Condor dataset ({\color{red}see Supplementary Figure X}). 
Thus an important part of the `repeat offender' groups and pages obtained from the Condor URLs were actually the same as the accounts analyzed previously ({\color{red}see Supplementary Figure X}). 
The point of this new analysis was to replicate the previous results with a more complete URL dataset and for this reason, we excluded the accounts whose engagement was already shown in the previous section. 
We thus show here the results for 503 `novel' accounts: 476 groups and 27 pages.

\subsection{Results}

Our first objective is to see whether the repeat offender policy was again applied only to Facebook pages, and not to groups.
For this, we calculate the percentage change in engagement between the `repeat offender' and the `no strike' periods for each of the 437 Facebook accounts that have published at least one post during each period ({\color{red}see Figure 5}). 
The median percentage change is $-5\%$, and the values are not significantly different from zero (W = $46495$, p-value = $0.61$).

The changes in engagement are found to be different for the groups and the pages. 
The median percentage change for the 414 Facebook groups is $-2\%$, while the median for the 23 Facebook pages is $-62\%$ ({\color{red}Figure 5}). 
A Wilcoxon test applied only to the percentage changes of the Facebook pages shows they are significantly different from zero (W = $29$, p-value = $0.00041$).

{\color{red}ADD FIGURE}

As in the previous section, we then analyzed the engagement received by the 503 repeat offenders accounts in 2019 and 2020 ({\color{red}see Figure 6}). 
The `novel' accounts replicated the slow rise in total engagement from September 2019 to June 2020, and the massive drop around June 9, 2020.

{\color{red}ADD FIGURE}

The percentage change in engagement was then calculated for each account during a 30-day period before and after June 9, 2020 ({\color{red}Figure 7}).
The median percentage change is $-26\%$, and $63\%$ of the accounts experienced a decrease in engagement, the results being a little more modest than what was found previously.
The values are still significantly different from zero (W = $42651$, p-value = $3.8 \times 10^{-5}$).
The 23 pages have a median percentage change of $-2\%$ (not significantly different from zero: W = $133$, p-value = $0.89$), while the 442 groups have a median percentage change of $-27\%$.

{\color{red}ADD FIGURE}

%\begin{enumerate}[(1)]
%\item Group the authors per affiliation.
%\item Use footnotes to indicate the affiliations.
%\end{enumerate}

\bibliography{mybibfile}

\end{document}
