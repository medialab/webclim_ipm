\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{xcolor}
\usepackage[labelfont=bf]{caption}
\modulolinenumbers[5]

\journal{Journal of Information Processing and Management }

\bibliographystyle{elsarticle-num}

\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

\begin{document}

\begin{frontmatter}

\title{Investigating Facebook's interventions against accounts that repeatedly share misinformation}

\author[mymainaddress]{Héloïse Théro\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding authors.}
\ead{thero.heloise@gmail.com}

\author[mymainaddress]{Emmanuel M. Vincent\corref{mycorrespondingauthor}}
\ead{emmanuel.vincent@sciencespo.fr}

\address[mymainaddress]{médialab - Sciences Po, Paris, France}

\begin{abstract}

Like many web platforms, Facebook is under pressure to regulate misinformation.
According to the company, users that repeatedly share misinformation (`repeat offender') will have their distribution reduced, but little is known about the implementation or the impacts of this measure.
This paper investigates the implementation and consequences of this policy using a first of its kind analysis, combining data from a fact-checking organization (Science Feedback), Facebook’s Social Science One dataset (Condor), users’ self-declaration and CrowdTangle data. 
Based on Science Feedback data, we first identified a set of public accounts (groups and pages) that have shared misinformation repeatedly during the 2019-2020 period.
The engagement per post decreased for Facebook pages after they shared two or more `false news’, and this result was replicated using Condor data.
We also discover that Facebook groups have been affected in a different way with a sudden drop in their average engagement per post that occurred around June 9, 2020.
Finally we identified a set of pages claiming to be under `reduced distribution' by Facebook for repeatedly sharing misinformation, and we again observed a decrease in their engagement per post.
In the three sets of pages studied, the median decrease in engagement after sharing misinformation is ranging from $-62\%$ to $-24\%$.

\end{abstract}

\begin{keyword}
Misinformation\sep Content moderation\sep Algorithmic transparency\sep Facebook\sep Fact-checking\sep Social media analysis
%\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}

The general public is increasingly getting news related information online, through search engines, social media and video platforms \citep{mitchell2016modern}.
Hence the spread of misinformation through these platforms has recently received growing attention.
Recent studies, along with the political context of January 2021 in the United States, show how the presence of misinformation online can contribute to negative societal consequences.
Namely it can fuel false beliefs, such as the idea of a massive voter fraud during the US 2020 presidential election, which may have led to the January 6, 2021 insurrection at the U.S. Capitol \citep{benkler2020mail} and other false stories about presidential candidates \citep{allcott2017social}. 
Misinformation has also contributed to confusing the public about the reality of climate change \citep{brulle30years, porter2019can} and stoked skepticism about vaccine safety among the public \citep{featherstone2020feeling, lahouati2020spread, pierri2021impact}. 
In April 2020, a questionnaire from the Reuters Institute found that people in the UK use online sources more often than offline sources when looking for information about the coronavirus. 
Among social media platforms, Facebook was the most widely used with $24\%$ of the respondents saying they used Facebook to access COVID-19 information in the last seven days \citep{fletcher2020information}.
The importance of Facebook in the media landscape is confirmed by Parse.ly’s dashboard, which shows that $25\%$ of the visitors of 2500+ media websites are referred by Facebook \citep{parslyDashboard}.

Lawmakers and regulators are increasingly pressuring platforms to limit the spread of misinformation. 
In the US, the House of Representatives organized hearings and convened representatives of the main platforms to testify on how they are being weaponized to spread ``misinformation and conspiracy theories online'' \citep{donovan2020}. 
In Europe, the European Commission has established a `Code of Practice on Disinformation' \citep{codePracticeMisinformation} that enjoins platforms to voluntarily comply with a set of commitments \citep{heldt2019let}.
Platforms' compliance with the Code of Practice is subjected to an annual assessment by the Commission, the first of which was released in September 2020 \citep{firstEUassess}.
The actions that platforms claim to be taking include limiting political advertisement or providing transparency regarding who is funding political advertising, promoting `authoritative' sources of information, providing data for researchers, sponsoring media literacy initiatives or informing users when they are interacting with misinformation \citep{selfassessment}.
However, there is little data available and few established processes to monitor the implementation of these measures and quantify their actual impact. 
Here we propose a methodology to monitor Facebook’s implementation of its policy to reduce the visibility of accounts repeatedly spreading misinformation. 
We chose to focus on Facebook as it is the biggest social media platform with more than two billion users worldwide.

Facebook announced a three-part policy to address ‘misleading or harmful content’: they claim to \textit{remove} harmful information, \textit{reduce} the spread of misinformation and \textit{inform} people with additional context \citep{threePartRecipe}. 
Facebook has developed the most extensive third-party fact-checking program with dozens of partner institutions to assist the company in this endeavour \citep{60factCheckingPartners}.
Fact-checkers have access to a stream of viral and likely problematic content, which they can verify and flag as misinformation, with options ranging from ``True'' (not misinformation), to ``Missing context'' to ``Partly false'' and ``False'' \citep{RatingOptions}.
Facebook informs page or group owners when published posts on their pages or groups are marked as misinformation, inviting them to correct the posts.
The platform's users receive a notification when they have shared a post marked as misinformation and see a notice linking to the fact-check over the flagged posts. 
A handful of papers provide evidence that supports the efficacy of fact-checking labels by reducing the likelihood that users share false information \citep{mena2020cleaning} and reducing false beliefs \citep{porter2021global}. 
In an experimental setting, Pennycook et al. \cite{pennycook2020understanding} show that prompting people to consider the accuracy of a piece of information increases the quality of the information they subsequently share on social media.
Facebook states that the virality of the posts marked as `False' or `Partly False' will be reduced.

The \textit{reduce} policy is not only applied to individual posts, but also to organizations that often publish posts containing misinformation, according to statements in Facebook’s publishers help center \citep{factCheckingRules, repeatOffenderCommunication}:
\begin{quote}
\emph{Pages and websites that repeatedly share misinformation rated False or Altered will have some restrictions, including having their distribution reduced.}
\end{quote}

Facebook ranks each post in users’ newsfeed by assigning a relevance score to it. 
A high score leads to a high likelihood of the post appearing at the top of a user's newsfeed. 
By decreasing the relevance score, Facebook can make a post or an entire account less visible \citep{threePartRecipe}.
However, Facebook has not provided data showing how their {\it reduce} policy is implemented that would allow researchers to quantify its impact on the spread of misinformation. 

One study analysed the reach of a set of websites identified as sources of false stories on Facebook and Twitter from January 2015 to July 2018. 
They found that during the 2016 American elections, total engagement on Facebook and Twitter for these sites had more than doubled compared to pre-election levels. 
Following the election, however, Facebook engagements fell sharply, while Twitter shares continued to increase for these sites, suggesting that Facebook might have taken measures to contain misinformation while Twitter did not \cite{allcott2019trends}. 

A more recent article by Kornbluh et al. (2020) \cite{kornbluh2020new} measured the level of interactions on Facebook with articles from outlets that repeatedly publish false content from 2016 to 2020, and found results contrasting with those of Allcott and colleagues \cite{allcott2019trends}. 
Although Kornbluh et al. did observe a decrease in the first and second quarter of 2017, they observed that total interactions with ‘deceptive’ outlets on Facebook have increased since then, and were $242\%$ higher during the third quarter of 2020 than during the run-up to the 2016 election. 
These results suggest that Facebook’s policy did not make a lasting impact on misinformation. 

Another similar approach was developed by Resnick and colleagues \cite{resnick2018iffy} in the form of the Iffy quotient: a daily calculation of the fraction of the 5,000 most popular URLs on a platform that came from `iffy' sites (made of a large list of sites that are defined as frequent sources of misinformation and hoaxes). 
According to this quotient, the proportion of top viral links on Facebook from `iffy' websites was about $20\%$ during both the 2016 and 2020 US presidential elections. 
On Twitter, the Iffy quotient increased from about $15\%$ in late October 2016 to around $20\%$ in late October 2020 \cite{IffyQuotient}. 
The three studies mentioned above find rather different results due to different methodologies and use of sources that they labeled `unreliable', but they paint a picture that is in agreement with a persistence of misinformation on Facebook and Twitter at an elevated level.

The present research article departs from articles studying the overall levels of misinformation on platforms by focusing on monitoring a specific policy against misinformation.
To that end, we used CrowdTangle, a public insights tool owned and operated by Facebook, to access Facebook data \citep{team2020crowdtangle}. 
CrowdTangle exclusively tracks public content, and provides access to engagement metrics (such as the number of likes, shares and comments), but not to the reach (number of views) of content \citep{helpCT}.
If Facebook decreases the visibility of posts from accounts sharing misinformation, we expect that their reach decreases.
As less users see these posts, the engagement per post should also decrease.
To investigate the effect of the {\it reduce} policy, we used the engagement per post as a proxy for the visibility of the `repeat offender' accounts content.
 
We first combined data from one of Facebook's fact-checking partners (Science Feedback) identifying URLs sharing misinformation and from CrowdTangle tracking engagement metrics of the Facebook accounts that repeatedly shared such misinformation.
We then replicated this methodology using a set of URLs marked as misinformation by more than one fact-checking organization obtained directly from Facebook (using the Condor dataset).
Finally, we investigated the engagement metrics of a set of Facebook pages claiming to be under reduced distribution.

\section{Research questions}

\begin{itemize}
\item Is Facebook’s policy aiming to reduce the distribution of misinformation from repeat offenders enforced and can its implementation be verified using available engagement data?
\item If implemented, what is the magnitude of the reduction in engagement metrics and how does it affect Facebook groups and pages?
\item What is the overall impact of the policy on the spread of misinformation on Facebook, i.e. does it result in a  decrease in engagement integrated for all repeat offender’s accounts over time?
\end{itemize}

\section{Investigating the reduce policy on Facebook accounts repeatedly sharing misinformation (Science Feedback data)}

To investigate a possible impact of Facebook’s policy against accounts that repeatedly share misinformation, we first identified such accounts using data from Science Feedback, one of Facebook’s third-party fact-checking partners \cite{sciencefeedbackFbPartner}.
Science Feedback is a fact-checking organization dedicated to verifying the credibility of science-related claims and articles. 
The organization tracks the most viral press articles or social media posts, invites scientists with domain expertise to evaluate their credibility and publishes explanatory articles for a general audience. 
It contributes to maintaining a database of URLs where the claims checked have been published or repeated that is available online at \url{open.feedback.org}.

\subsection{Methods}

We obtained from Science Feedback a list of 4,000+ URLs reviewed by its team. 
The list was obtained on January 4, 2021 and cover links flagged in 2019 and 2020.
We relied on the 2,452 URLs marked as `False', which we refer to as `false news links', excluding the URLs marked as `Partly False', `Missing Context', `False headlines' or `True', as well as the URLs marked as `False' but `corrected' by the publisher, because these labels do not contribute to the `repeat offender' status according to Facebook's guidelines.
Sharing a URL flagged as `Altered' also contributes to the `repeat offender' status \citep{factCheckingRules, repeatOffenderCommunication}, but we found no such rating in Science Feedback data.

Using the `/links' endpoint from the CrowdTangle API, we collected the public Facebook groups and pages that shared at least one false news link between January 1, 2019 and December 31, 2020. 
Due to the API limitations, if a URL was shared in more than 1000 posts, we collected only the 1000 posts that received the highest number of interactions \cite{docCT}; this means that we miss some of the accounts that generate the least interactions and our results focus on the most prolific accounts.
We focused on the accounts that spread misinformation the most often, choosing a threshold of 24 different false news links shared over the two years period. 

The corresponding 307 Facebook accounts (289 Facebook groups and 18 Facebook pages) are referred to as `repeat offenders accounts'. 
The list of accounts is available on the paper’s GitHub repository: {\color{red} \url{https://github.com/medialab/webclim_facebook/blob/master/data/crowdtangle_list/account_list_part_1.csv}}.
The repository also contains the code used to collect and analyze the data, and to plot the figures.
All the posts they published between January 1, 2019 and December 31, 2020 were collected using the `/posts' endpoint. 
We calculated the engagement per post by summing the number of comments, shares and reactions (such as ‘like’, ‘love’, ‘favorite’, ‘haha’, ‘wow’, ‘sad’ and ‘angry’ reactions) that each post has received.

`Repeat offender' accounts are supposed to have their distribution reduced, according to Facebook's official communication, but the precise rule Facebook uses to classify an account as `repeat offender' is not specified. 
However, a Facebook’s staff indicated to a journalist \cite{2strikes90daysRule} that:
\begin{quote}
\emph{The company operates on a `strike' basis, meaning a page can post inaccurate information and receive a one-strike warning before the platform takes action. 
Two strikes in 90 days places an account into `repeat offender' status.}
\end{quote}

Based on this `two strikes in 90 days' rule and the list of strike dates known by Science Feedback, we inferred periods during which each account must have been under repeat offender status. 
If a post shares a misinformation link which was previously fact-checked as `False', we used the date of the post as the strike date. 
However, if an account shares a link, which later gets fact-checked as `False', then the fact-check date was used as the strike date. 
A repeat offender period is defined as any given time in which an account shared two or more `false news links' over the past 90 days (see Figure \ref{repeat_example_timeseries} for two examples).

The set of accounts analysed comprises some groups and pages that generate more engagement than others by several orders of magnitude. 
Because the underlying distribution of the engagement metrics were not Gaussian, we used non-parametric statistical methods. 
To compare the engagement metrics between different periods, we calculated the percentage change for each account, and tested their difference against zero with a Wilcoxon test.
As it compares the sums of ranks, a Wilcoxon test is less likely than a t-test to spuriously indicate significance because of the presence of outliers \cite{wilcoxon1992individual}.

The confidence intervals around the medians are estimated using a non-parametric approach, called bootstrap. 
In each range, the engagement metrics for the N accounts are called events. 
We randomly sample N events with replacement, meaning that one event can be selected more than once. 
From the selected N events, we calculate the median. 
By repeating this process 1,000 times, we obtain 1,000 values for the median. 
The upper and lower limit of each error bar (in Figures \ref{repeat_vs_free_percentage_change}, \ref{repeat_june_drop_percentage_change}, \ref{condor_repeat_vs_free_percentage_change}, \ref{condor_june_drop_percentage_change} and \ref{reduce_percentage_change}) represents the 5\% and 95\% percentiles of the 1,000 medians.


\subsection{Results}

Figure \ref{repeat_example_timeseries} displays the engagement metrics for one `repeat offender' group named \href{https://www.facebook.com/groups/108655705888371/}{`Australian Climate Sceptics Group'} and one `repeat offender' page named \href{https://www.facebook.com/avoiceforchoice/}{`A Voice for Choice'}.
The known strike dates appear as red lines at the bottom and the inferred ‘repeat offender’ periods are shaded in red.
The average engagement per post varies throughout the past two years, but does not appear to be related with the shift between `repeat offender' and `no strike' periods for `Australian Climate Sceptics Group'.
For the `A Voice for Choice' page, we observe a decrease in engagement in 2020, as the page repeatedly shared different False URLs, which would have maintained it under `repeat offender' status throughout 2020.
We compared the average engagement metrics between the `repeat offender' and the `no strike' periods, expecting a decrease in engagement during the `repeat offender' periods.
The percentage change is $+61\%$ for `Australian Climate Sceptics Group', and $-58\%$ for `A Voice for Choice'.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/sf_examples_timeseries.png}
\caption{
\textbf{(Top panel)} Average engagement (the sum of comments, shares, likes, ...) per post for the `Australian Climate Sceptics Group' Facebook group for each day in 2019 and 2020.
Each red line at the bottom represents the date of a known strike for this group according to Science Feedback data. 
The areas shaded in red represent the `repeat offender' periods as defined by the ‘two strikes in 90 days’ rule.
\textbf{(Bottom panel)} Same as above for the `A Voice for Choice' Facebook page.
}
\label{repeat_example_timeseries}
\end{figure}

To provide a general overview, we calculate the percentage change between the `repeat offender' and the `no strike' periods for each of the 256 Facebook accounts that have published at least one post during each period (see Figure \ref{repeat_vs_free_percentage_change}).\footnote{The percentage changes were calculated on the periods between January 1, 2019 and June 8, 2020. Because of the drop in engagement described further down, the second semester of 2020 was excluded (see Figure \ref{repeat_average_timeseries}).}
The median percentage change is $-6\%$, and a Wilcoxon test shows that the values are not significantly different from zero (W = $16051$, p-value = $0.74$).

When we consider groups and pages separately, the percentage changes are different for the two.
For the 238 Facebook groups, the percentage changes are not significantly different from zero (W = $13561$, p-value = $0.54$), with a median of $-3\%$, while for the 18 Facebook pages, the percentage changes are significantly different from zero (W = $21$, p-value = $0.0034$), with a median of $-43\%$.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/sf_repeat_vs_free_percentage_change.png}
\caption{
Percentage changes between the average engagement per post during the `repeat offender' periods and the `no strike' periods.
Each deep blue dot represents a Facebook group, and each light blue dot a Facebook page.
The bars show the medians for each set and their $90\%$ confidence intervals (the intervals are estimated using a bootstrap method).
The 256 `repeat offender' accounts represented here were identified using Science Feedback data, and have published at least one post during each period.
}
\label{repeat_vs_free_percentage_change}
\end{figure}

To see whether the strikes would otherwise influence the repeat offenders accounts' engagement over time, we analyzed the total amount of engagement received by all the posts published by each of the 307 repeat offenders accounts for each day of the 2019-2020 period (Figure \ref{repeat_average_timeseries}). 
This metric, representing the total engagement generated by these accounts on Facebook (top panel), can be decomposed as the number of posts published each day (middle panel) times the average number of engagement per post (bottom panel).

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/sf_average_timeseries.png}
\caption{
Metrics aggregated over the 307 Facebook accounts that repeatedly shared false news links identified using Science Feedback data.
{\bf(Top panel)} Total engagement per day averaged for all accounts.
{\bf(Middle panel)} Number of posts per day. 
{\bf(Bottom panel)} Average engagement per post. 
The dotted red line marks the date of June 9, 2020, when a sudden drop in engagement is observed.
}
\label{repeat_average_timeseries}
\end{figure}

The total engagement per day is stable from January to September 2019, however we observe a rise from September 2019 to June 2020. 
This rise is explained by the increase in activity of the misinformation accounts (with a doubling of the number of posts per day) while the engagement per post remained rather constant.
Around June 9, 2020, the total engagement metrics have massively dropped.
This decrease is entirely explained by a corresponding drop in engagement per post (Figure \ref{repeat_average_timeseries}).
While this drop has cut the groups' engagement per post in half, this decrease was compensated by the fact that `repeat offender' accounts have doubled their number of posts between 2019 and 2020.
The net result is that the total engagement on posts from `repeat offender' accounts returned to its early 2019 levels.

To further quantify this ‘June drop’, we calculated the percentage change in engagement per post for each account during a 30-day period before and after June 9, 2020 (Figure \ref{repeat_june_drop_percentage_change}). 
The median percentage change is $-43\%$, and most of the accounts (219 out of 289) experienced a decrease in engagement\footnote{A decrease in engagement on June 9, 2020 can be seen for the `Australian Climate Sceptics Group' in Figure \ref{repeat_example_timeseries} (the percentage change was $-60\%$ for this example).}.
A Wilcoxon test indicates that these percentage changes are significantly different from zero (W = $9012$, p-value = $4.6 \times 10^{-17}$).

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/sf_june_drop_percentage_change.png}
\caption{
Percentage changes in the average engagement per post during a 30-day period before and after June 9, 2020. 
Each deep blue dot represents a Facebook group, and each light blue dot a Facebook page.
The bars show the medians for each set and their $90\%$ confidence intervals.
The 289 `repeat offender' accounts represented here were identified by Science Feedback data, and have published at least one post one month before and one month after June 9, 2020.
}
\label{repeat_june_drop_percentage_change}
\end{figure}

When we consider groups and pages separately, the percentage changes are different for the two.
While the percentage changes for the 271 groups are significantly different from zero (W = $7599$, p-value = $5.1 \times 10^{-17}$), with a median of $-45\%$,
the 18 pages appear to not be affected by the decrease (W = $73$, p-value = $0.61$), with a median percentage change of $-5\%$. 
As the June drop does not affect groups and pages equally, we reproduced Figure \ref{repeat_average_timeseries}’s bottom panel for groups and pages separately (see Supplementary Figure \ref{engagement_groups_and_pages_sf}), which further shows that the June 2020 engagement metrics’ drop only affects groups.

To verify whether this drop was specific to `repeat offender' groups, we compared these dynamics to those of a control set of accounts consisting of Facebook pages and groups associated with accounts that did not publish misinformation.
No such drop in engagement was observed around June 9, 2020 (see Supplementary Figure \ref{june_drop_control}).

The most likely explanation for such a massive change is that Facebook modified how its algorithm promoted the content from these groups starting on June 9, 2020.
While we did observe a relationship between the strike dates and a decrease in engagement for `repeat offender' pages, we observed no such link for `repeat offender' groups.
Hence it seems that Facebook took action against these groups via this one-shot measure in June 2020.

\section{Investigating the reduce policy on accounts repeatedly sharing misinformation (Condor data)}

\subsection{Methods}

We used data from the Social Science One organization \cite{king2020new}, that builds partnerships between academia and private industries such as Facebook to share data and expertise. 
In July 2021, we had access to a new version of the Condor dataset \cite{messing2020facebook}, which contains all URLs shared publicly by at least 100 Facebook users between 2017 and 2021, as well as their fact-checking metadata. 
From this list, we extracted the 6,811 URLs that were shared in 2019 and 2020, that were fact-checked as `False' and whose country in which it was shared most frequently was either the USA, Canada, Great Britain or Australia.

We then replicated as closely as possible the methods used in the previous section. 
Using CrowdTangle, we thus collected all the posts that shared one of the false links between January 1, 2019 and December 31, 2020, and focused on the 706 Facebook accounts (671 Facebook groups and 35 Facebook pages) that spread at least 24 false links. 
Then we used CrowdTangle again to collect all the posts published by those accounts in 2019 and 2020. 
Because the Condor dataset contained the date of the first fact-check done on a URL, we were able to infer the `repeat offender' periods for each account and therefore conduct the same analysis as in the previous section.

Science Feedback being a third-party fact-checker working with Facebook, some of the URLs from Science Feedback are also contained in the Condor dataset (see Supplementary Figure \ref{venn_urls}). 
Thus a significant part of the `repeat offender' groups and pages obtained from the Condor URLs are actually the same as the accounts analyzed previously. 
As the point of this new analysis is to replicate the previous results, we exclude the accounts whose engagement was already shown in the previous section. 
We thus show here the metrics for only the 503 `novel' accounts, which represent 476 groups and 27 pages (see Supplementary Figure \ref{venn_accounts}).

\subsection{Results}

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/condor_repeat_vs_free_percentage_change.png}
\caption{
Same metric as on Figure \ref{repeat_vs_free_percentage_change}
The 437 `repeat offender' accounts represented here were identified using Condor data, and have published at least one post during each period.
}
\label{condor_repeat_vs_free_percentage_change}
\end{figure}

Our first objective is to verify that the repeat offender policy was applied only to Facebook pages, and not to groups during the 2019-2020 period.
To do this, we calculate the percentage change in engagement between the `repeat offender' and the `no strike' periods for each of the 437 Facebook accounts that have published at least one post during each period (see Figure \ref{condor_repeat_vs_free_percentage_change}). 
The median percentage change is $-5\%$, and the values are not significantly different from zero (W = $46495$, p-value = $0.61$).

The changes in engagement are also different for the groups and the pages (Figure \ref{condor_repeat_vs_free_percentage_change}). 
The percentage changes for the 414 Facebook groups are not different than zero (W = $41561$, p-value = $0.57$), with a median of $-2\%$, while the values for the 23 Facebook pages are significantly different than zero (W = $29$, p-value = $0.00041$), and the median is $-62\%$.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/condor_average_timeseries.png}
\caption{
Same metrics as on Figure \ref{repeat_average_timeseries} aggregated over the 503 `repeat offender' Facebook accounts identified using Condor data.
}
\label{condor_average_timeseries}
\end{figure}

As in the previous section, we then analyzed the engagement received by the 503 repeat offenders accounts in 2019 and 2020 (see Figure \ref{condor_average_timeseries}). 
The `novel' accounts replicated the slow rise in total engagement from September 2019 to June 2020, and the massive drop around June 9, 2020.
Again, we observe that this measure set the engagement for ‘repeat offenders’ groups back to its early 2019 level.

The percentage change in engagement was then calculated for each account during a 30-day period before and after June 9, 2020 ({Figure \ref{condor_june_drop_percentage_change}).
The median percentage change is $-26\%$, and $63\%$ of the accounts experienced a decrease in engagement, the results being a little more modest than what was found previously.
The values are still significantly different from zero (W = $42651$, p-value = $3.8 \times 10^{-5}$).

When tested separately, the percentage changes for the 442 groups are significantly different from zero (W = $37889$, p-value = $3.8 \times 10^{-5}$) and the median is $-27\%$, whereas the values for the 23 pages are not different from zero (W = $133$, p-value = $0.89$), with a median of $-2\%$.
Also, when the engagement per post is plotted separately for groups and pages, we can see a drop in engagement only for groups (see Supplementary Figure \ref{engagement_groups_and_pages_condor}).

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/condor_june_drop_percentage_change.png}
\caption{
Same metric as on Figure \ref{repeat_june_drop_percentage_change}.
The 465 `repeat offender' accounts represented here were identified using Condor data, and have published at least one post one month before and one month after June 9, 2020.
}
\label{condor_june_drop_percentage_change}
\end{figure}

To conclude, using a more complete dataset of `False' URLs and collecting new Facebook accounts, we replicated our previous findings. 
Indeed we again find a sudden decrease in engagement for repeat offender Facebook groups in June 2020, and a decrease in engagement following the publication of two false links for repeat offender Facebook pages.

One limitation of the results is that this king of analysis is rather indirect, as we relied on the strike dates to infer the `repeat offender' periods, and we cannot know for certain whether the pages investigated were actually under a `repeat offender' status. 
For example, one could imagine that the `two strikes in less than 90 days' rule may have changed over time, or that links fact-checked as `partly false' or `missing context' were also counted as strikes (only links fact-checked as `False' were taken into account in our analysis).
In the next section, we used a different methodology to collect pages for which we are sure that they are under `repeat offender' status.

\section{Investigating the reduce policy on pages declaring to be under `reduced distribution'} 

\subsection{Methods}

We noticed that two popular pages (`Mark Levin' and `100 Percent FED Up') have publicly shared a message claiming to be placed under `repeat offender' status with a screenshot as a piece of evidence.
To gather a list of such self-declared repeat offenders, we searched on CrowdTangle for posts published since January 1, 2020 with the following keywords:
\begin{itemize}
\item `reduced distribution' AND (`restricted' OR `censored' OR `silenced')
\item `Your page has reduced distribution'
\end{itemize}
For this we used the `/posts/search' endpoint of the API on November 25, 2020. 

We manually opened the resulting posts, and kept the ones which met the following criteria (see Figure \ref{reduce_example} top panel for an example):
\begin{itemize}
\item The post should include a screenshot of the Facebook notification.
\item In the screenshot, the Facebook notification should say: `Your page has reduced distribution and other restrictions because of repeatedly sharing of false news.'
\item In the screenshot, the name of the page should be visible.
\end{itemize}

Doing so, we obtained a list of 94 pages. 
We found only Facebook pages in this case, and no groups. 
A search using the terms `Your group has reduced distribution' did not yield any result.

To verify whether Facebook applied any restriction to these pages, we collected all the posts that these 94 pages have published between January 1, 2019 and December 31, 2020 from the CrowdTangle API using the `/posts' endpoint. 
The collection was run on January 11, 2021.
We were only able to collect data from 83 of these pages, as 11 were deleted from the CrowdTangle database since our search in November 2020. 
This highlights an important issue when studying misinformation trends on Facebook: some data disappears as accounts are deleted or changed to ‘private’.

Among the 83 Facebook pages collected, two were already among the 18 pages included in the first analysis, and one was already present in the set of 35 pages included in the second analysis (see Supplementary Figure \ref{venn_accounts}).
We excluded these pages to present only the 80 `novel' pages in this section.

The date of the last fact-check notification was used as the inferred start date of reduced distribution, when it appeared in the screenshot. 
When it was not visible, we used the date of the post as the inferred start date of reduced distribution.
The inferred `reduced distribution' dates range from April 1rst to November 23, 2020.
We are aware that the inferred date may not correspond to the real date at which the restrictions has begun to be enforced.
For example, a page may have received a `reduced distribution' notification from Facebook in early 2020, while sharing a screenshot of this notification only a few months later.
Because the `reduced distribution' notification is a private message that cannot be accessed unless the page shares it publicly, we had no choice but to rely on this inferred date as a proxy for the start date of the restrictions.

\subsection{Results}

\begin{figure}[!h]
\centering
\includegraphics[scale=0.15]{./../figure/reduce_example_screenshot.png}
\includegraphics[scale=0.5]{./../figure/reduce_example_timeseries.png}
\caption{
{\bf(Top panel)} Screenshot of a \href{https://www.facebook.com/100PercentFEDUp/photos/a.330374477016724/3201797096541100}{post from the `100 Percent FED Up' Facebook page} sharing a `reduced distribution' notification from Facebook (screenshot taken on September 22, 2021). 
{\bf(Bottom panel)} Average engagement per post for the `100 Percent FED Up' page for each day in 2019 and 2020.
The dotted red line represents the reduced distribution start date that is infered from the date of the last violation on the screenshot (`Notified July 31, 2020').
}
\label{reduce_example}
\end{figure}

Figure \ref{reduce_example} shows a screenshot of the Facebook notification shared by the `100 Percent FED Up' page (with a last violation notified on July 31, 2020), and the average engagement per post of that page over the past two years. 
We see a clear decrease in engagement in August 2020, and when we compare the engagement during a 30-day period before and after July 31, 2020, the percentage change is $-62\%$.

To provide a general overview, we calculate the percentage change in engagement during a 30-day period before and after the reduced distribution start date for each of the 79 Facebook pages that published at least one post during each period (see Figure \ref{reduce_percentage_change}).
The median percentage change is $-25\%$, and a Wilcoxon test reveals that the percentage changes are significantly different from zero (W = $855$, p-value = $0.00040$).
We can thus suggest that the `reduced distribution' status is associated with a modest decrease in engagement.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/reduce_percentage_change.png}
\caption{
Percentage changes in average engagement per post during a 30-day period before and after the reduced distribution start date. 
Each dot represents a Facebook page. 
The bars show the median and its $90\%$ confidence interval.
The 79 `reduced distribution' pages represented here were identified because they shared a `reduced distribution' notification from Facebook in 2020.
}
\label{reduce_percentage_change}
\end{figure}
 
Finally, we verify whether an important drop in engagement also occurred in June 2020 for this set of Facebook pages.
When we compare the engagement metrics before and after June 9, 2020, the median percentage change is $4\%$. Although the difference from zero is marginally significant (W = $992$, p-value = $0.049$), it means that the engagement per posts tended to {\it increase} after June 2020 for these pages (also see Supplementary Figure \ref{engagement_groups_and_pages_reduce} to observe the engagement dynamics).
This confirms that Facebook pages have most likely not been affected by the sudden {\it reduce} measure implemented in June 2020 and evidenced in the previous sections.

\section{Discussion}

Facebook, the most widely used social media platform in the world, has announced a series of measures to curb the spread of misinformation, notably by reducing the visibility of `repeat offenders', which are accounts that repeatedly share false information. 
However, the effects of the platforms' diverse policies to tackle misinformation remains understudied \citep{pasquetto2020tackling}. 
The present research article aims to contribute to filling this knowledge gap by verifying the application and measuring the consequences of Facebook's `reduce' policy on the targeted accounts' engagement metrics.

As a first step, we investigated 307 Facebook accounts (mainly groups) having repeatedly shared misinformation using a fact-checker's dataset. 
Sharing two false links over a three-month period is supposed to be penalized by a reduced visibility of the account's content \cite{2strikes90daysRule}. 
We did observe a significant decrease (median of $-43\%$) in the engagement per posts published by pages under a presumptive repeat offender status.
However, we find no evidence that this policy is leading to a significant decrease in engagement for Facebook groups.

As a second step, we replicated this methodology using another dataset of URLs shared by Facebook, and identified 503 novel accounts sharing misinformation. 
We again observed a significant decrease (median of $-62\%$) in engagement for `repeat offender' pages, while the engagement for `repeat offender' groups remained stable.
 
As a third step, we identified 83 Facebook pages which have shared a Facebook notification, indicating that their account was under reduced distribution.
The pages' engagement metrics were significantly lower after the date of the notification (median of $-25\%$), suggesting that the `reduced distribution' measure was indeed applied to the pages.
We noted that no group was found when searching for accounts sharing a reduced distribution notification, which confirms that the `repeat offender' policy is applied only to Facebook pages, and not to groups.

The different methodologies we used to infer the repeat offender periods are subjected to biases.
On one hand, to identify perfectly repeat offender periods from a list of False URLs, we should have access to all the False URLs that this given accout has shared, and the two URL data sources we relied on are both more or less incomplete.
Moreover the `two strikes in 90 days' rule used to infer repeat offender periods may not be the exact rule used by Facebook, or the rule may have changed with time.
On the other hand, the date at which a page shared their `reduced distribution' notification may be months later after the page received the notification.

There is no public data indicating when a group or page is under a repeat offender status, as this information is a private notification sent to Facebook to the concerned accounts only.
The only way to monitor the repeat offender policy was thus to deduce which groups or pages should be reduced from the data available to us.
We found consistent results in the three different analyses, with a median decrease in engagement ranging from $-62\%$ to $-24\%$, and it hints that these different methods estimated more or less correctly the repeat offender periods.
Furthermore there might be a lag between the strike date and the application of the `repeat offender' policy, to allow for human verification. 
If so, it should be of a few days and have minimal impact on our calculations using 30 days or more windows.
Because of these potential biases, it is likely that the true size effect of the `repeat offender' interventions is underestimated.

Although we observe a global reduction in engagement, there is a large heterogeneity across the different `repeat offender' pages (see Figures \ref{repeat_vs_free_percentage_change}, \ref{condor_repeat_vs_free_percentage_change} and \ref{reduce_percentage_change}). 
The engagement of some popular pages have actually increased, such as the \href{https://www.facebook.com/TuckerCarlsonTonight/}{`Tucker Carlson Tonight' page} with a $38\%$ increase (from 104k to 143k interactions per post) following the `reduced distribution' notification from Facebook.
The engagement of the \href{https://www.facebook.com/marklevinshow}{`Mark Levin' page} remained rather stable after the notification (change of $2\%$), going from 20.3k to 20.7k interactions per post.
It is possible that these popular pages had their distribution already reduced for months before sharing the notification, or that they compensated the decrease in engagement led by the reduce intervention by a simultaneous gain of followers, but a recent article points toward an alternative explanation.
Some high-profile Facebook users such as celebrities, politicians and journalists might be exempted from the normal enforcement processes, according to company documents revealed by the Wall Street Journal \citep{WSJrevelations}.

By analyzing the time series of the repeat offenders’ engagement over the past two years, we also discovered a sudden drop affecting the groups around June 9, 2020.
For many groups, the decrease was quite drastic (up to $-70\%$ - $-80\%$), with a median drop in engagement of $-45\%$ for the first analysis and $-27\%$ for the second one.
The 18 Facebook pages from the first sample, the 23 pages from the second sample, as well as the 80 pages from the second sample, were not affected by this decrease.
This `June drop' does not correspond to any official communication by Facebook on that matter. 
It indicates that the company has very likely taken internal decisions that heavily impact the organic reach of repeat offenders' groups, in ways that differ from its stated policy against repeat offenders pages.
More transparency from Facebook would be needed to understand the nature and origin of this change. 
It would also bring clarity on how rules aimed at limiting the spread of misinformation are being enforced.

It is not clear why only repeat offender Facebook groups, and not pages, saw their engagement reduced in June 2020.
Studies have highlighted that misinformation persists at high levels on Facebook and other platforms \citep{kornbluh2020new, resnick2018iffy}.
In the context of the COVID-19 pandemic, concerns rose about the amount of misinformation spreading on social media, including Facebook, and its potential harm to users \citep{johnson2020online}.
It is possible that such concerns have driven Facebook to apply a `quick fix' to decrease the engagement of posts shared in groups spreading misinformation and compensate for the absence of a repeat offender policy.
One should note that since the overall activity in these misinformation groups doubled between September 2019 and June 2020, the `June drop' has only succeeded in bringing the overall engagement level back to its early 2019 values (see Figures \ref{repeat_average_timeseries} and \ref{condor_average_timeseries} top panels).

Facebook pages and groups have different purposes: pages are meant to be for official communication from the page administrators to a large audience, while groups are meant to foster interactions between users \citep{differenceGroupAndPage}. 
Pages are thus always public, while groups can be public or private.
Pages' posts can also be monetized and promoted.
Despite these differences, we have seen that both pages and groups are being used to share false news, and we actually found vastly more groups than pages when we identified the accounts spreading the most misinformation.
Indeed, groups represented 94\% of the accounts sharing at least 24 False URLs in the first analysis, and 95\% in the second analysis.
In the interest of curbing the spread of misinformation, applying its `repeat offender' policy to groups as well as to pages would have helped Facebook to decrease the amount of misinformation in their users’ feeds in 2019 and 2020.

It should be noted that fighting misinformation is a relatively new issue for platforms that appeared with the 2016 American elections, and the misinformation regulations are constantly changing.
It appears that Facebook is now also applying its reduce policy on misinformation groups, as one of the `repeat offender' group analyzed has shared in 2021 a `reduced distribution' notification from Facebook (see Figure \ref{screenshot_reduced_group}).

\begin{figure}[!h]
\centering
\includegraphics[scale=0.24]{./../figure/screenshot_reduced_group.png}
\caption{
Screenshot of \href{https://www.facebook.com/groups/mcbowwow/posts/3893068180741848/}{the post of a `repeat offender' group}, sharing in May 2021 a `reduced distribution' notification sent by Facebook.
}
\label{screenshot_reduced_group}
\end{figure}

The `reduced distribution' notification is different for groups and pages. 
Notably groups are informed by Facebook that: ``You can delete false information to help restore your group's distribution'' (see Figure \ref{screenshot_reduced_group}).
In contrast, page owners cannot get rid of a strike in the same way as group owners: ``Note that deleting a post will not eliminate the strike against the Page or domain'', although they can correct the posts and submit an appeal to fact-checkers for the strike to be lifted \cite{FacebookCorrectRating}.
As the followers of a group, and not just its administrators, can post in the group, group content can be hard to control. 
Maybe that is why this exception to the `repeat offender' restrictions was created for groups. 
We would nevertheless argue that it makes the policy easier to be circumvented for groups repeatedly sharing misinformation.
Furthermore, Facebook has announced in May 2021 that an individual's Facebook account will also be reduced if they repeatedly share misinformation content \cite{FacebookReduceUsers}.
It would thus be interesting to replicate our findings on the 2021 engagement data to monitor the effects of these new measures.

Online misinformation can be a threat to society, and the role that platforms can play via targeted interventions, has been the subject of intense debate over the past few years \citep{rogers2020deplatforming}. 
As a consequence, researchers \citep{mena2020cleaning, yaqub2020effects} and journalists \citep{FacebookPartisanBias, FacebookCivilityGrowth} have begun to monitor the actions that platforms take to tackle misinformation and their efficacy.
Given the facts that:
\begin{enumerate}[(1)]
\item false news go viral much faster than fact-checks can get published,
\item accounts that have shared misinformation in the past tend to keep sharing misinformation,
\item a small number of accounts is responsible for a large proportion of the misinformation being shared (at least regarding COVID-19 \citep{disinformationDozen}),
\end{enumerate}
then acting against `repeat offenders' is likely to be one of the most effective interventions that platforms can make to protect their users against manipulation.

There is a critical need for further research to thoroughly verify and shed light on platforms' actions against misinformation. 
While our results provide information on the relative drop in engagement per post resulting from Facebook’s repeat offenders policy, more research is needed to quantify the impact of such policies on the overall prevalence of misinformation in users’ feeds.

\bibliography{mybibfile}

\newpage

\beginsupplement

\textbf{SUPPLEMENTARY INFORMATION}

\section*{Engagement dynamics plotted separately for groups and pages}

In this article, we find a clear difference in how Facebook groups and pages are regulated by Facebook, which explains why the data is often plotted separately for these two kinds of accounts. 
The only exceptions are the engagement dynamics shown in Figures \ref{repeat_average_timeseries} and \ref{condor_average_timeseries}, in which all the `repeat offender' accounts - groups and pages - are reprensented together.
However, the June drop is only affecting Facebook groups and not pages, but this difference is not visible on the above mentioned figures representing all the accounts.

This is why we have plotted here the engagement per posts separately for groups and pages for the accounts identified using the Science Feedback dataset (see Figure \ref{engagement_groups_and_pages_sf}) and for the accounts identified using the Condor dataset (see Figure \ref{engagement_groups_and_pages_condor}). 
We can observe that the engagement per post do remain stable for the `repeat offender' pages in June 2020.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/supplementary_engagement_groups_and_pages_sf.png}
\caption{
Average engagement per post in 2019-2020 plotted separately for the 289 groups (top panel) and the 18 pages (bottom panel) identified as `repeat offender' using Science Feedback data.
}
\label{engagement_groups_and_pages_sf}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/supplementary_engagement_groups_and_pages_condor.png}
\caption{
Average engagement per post in 2019-2020 plotted separately for the 476 groups (top panel) and the 27 pages (bottom panel) identified as `repeat offender' using Condor data.
}
\label{engagement_groups_and_pages_condor}
\end{figure}

We also plotted here the engagement dynamics for the set of misinformation pages that shared a `reduced distribution' notification (see Figure \ref{engagement_groups_and_pages_reduce}).
As with the previous sets of pages shown above, there is no reduction in engagement for these pages in June 2020.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/supplementary_engagement_pages_reduce.png}
\caption{
Average engagement per post in 2019-2020 for the 80 `reduced distribution' pages, identified because they shared a `reduced distribution' notification from Facebook.
}
\label{engagement_groups_and_pages_reduce}
\end{figure}

These graphs illustrate that only Facebook groups, and not pages, were affected by the reduce measure implemented on June 9, 2020.

\section*{Engagement dynamics in 2019-2020 for a control set of accounts}

We compared the dynamics of the `repeat offender' accounts to those of a control set of accounts, which consisted of Facebook pages and groups associated with established news outlets that we expect to have received no false fact-checks on their posts.

To identify such a set, we used a report from NewsWhip \citep{NewsWhipReport} that identified the 10 media outlets that communicated the most during the early phase of the COVID-19 pandemic (first half of 2020), i.e., NBC, The Daily Mail, CNN, Fox News, The Independent, BBC, The New York Times, The Washington Post, Yahoo and The New York Post.
We searched the outlets' names on Facebook and created a list of 10 pages and six groups that displayed a verified `blue check'.
We also searched for more groups as they are the accounts supposed to be affected by the June drop.
We added to this set 19 Facebook groups created before June 2020 that are either associated with a media outlet (such as \href{https://www.facebook.com/groups/brexitlatest/}{the `Brexit latest - the Independent' group}) or that have a science focus (such as \href{https://www.facebook.com/groups/437670336804119/}{the `WE ARE SCIENTISTS' group}).
Using CrowdTangle, we collected all the posts published by these accounts between January 1, 2019 and December 31, 2020.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/supplementary_mainstream_june_drop_percentage_change.png}
\caption{
Same metrics as on Figure \ref{repeat_june_drop_percentage_change} aggregated over the `control' Facebook accounts that published at least once during 30 days before and after June 9, 2020.
}
\label{june_drop_control}
\end{figure}

The percentage changes after June 9, 2020 are not significantly different from zero for groups (W = $95$, p-value = $0.32$, median = $-7\%$) and for pages (W = $27$, p-value = $1$, median = $-6\%$, see Figure \ref{june_drop_control}).
Therefore, contrary to what we observe for the `repeat offender' groups, we found no drop in engagement in June 2020 for the `control' groups.
This observation further supports the hypothesis that the drop observed for the `repeat offender' groups is specifically targeted at these misinformation groups, and not a feature that broadly affected Facebook groups.

\section*{Overlap between the lists of false URLs}

In the two first methods, we used two different sources to get a list of False URLs fact-checked in 2019-2020, but there should be an overlap between these two lists. 
Indeed, Science Feedback is a third-party fact-checker partnering with Facebook \citep{sciencefeedbackFbPartner}, and the URLs fact-checked by Science Feedback were transfered to Facebook.
We can thus imagine that the list of URLs from Science Feedback would be included in the list from Condor.

However the only URLs that are in Condor are the ones shared by more than 100 users on Facebook, which excludes the less viral URLs fact-checked by Science Feedback.
Moreover, as Condor is one of the largest social science research dataset ever constructed, issues related to data quality, validity and fidelity are expected to be found \cite{messing2020facebook}.
For example it was recently revealed that the engagement data in Condor was only based on around half of the U.S. users and thus incomplete, because the views of the users that were not politically classified were not taken into account \citep{NYTrevelations}. 
Although this error should not impact the list of URLs we used in this article, other issues might have altered the list of False URLs, and that reason could also explain why some URLs from Science Feedback were excluded from the Condor list.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/supplementary_venn_urls.png}
\caption{
Overlap between the list of False URLs from Science Feedback and the list of False URLs from Condor.
}
\label{venn_urls}
\end{figure}

To compare the two lists of URLs, we first normalized all the URLs with the same method, and then built a Venn diagram from the two lists of normalized URLs (see Figure \ref{venn_urls}). 
The overlap was found to be smaller than expected. 
Indeed only $32\%$ of the URLs in Science Feedback were also in Condor, suggesting that most of the URLs fact-checked were shared by less than 100 users. 
Furthermore, the URLs from Science Feedback represented only $11\%$ of all the URLs in Condor. As Science Feedback is only one of the 60+ fact-checking organizations partnering with Facebook \citep{60factCheckingPartners}, we can see that its fact-checked URLs are actually well represented in the Condor dataset.

\section*{Overlap between the different sets of accounts analyzed}

As we used different methodologies to obtain three different lists of `repeat offender' accounts, we verify how much of these accounts were redundant in the different lists. 

In the third analysis only pages were found, and thus we only compare the lists of Facebook groups collected between the first and second analyses (see Figure \ref{venn_accounts} left panel). 
Although the lists of false URLs do not overlap that much between the two data sources, we found that two third ($67\%$) of the groups identified using Science Feedback data were also obtained from Condor data. 
This can be explained because the ULRs in common between Science Feedback and Condor were the most viral ones (shared by more than 100 users) and thus these URLs played an important role to identify accounts repeatedly sharing misinformation.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{./../figure/supplementary_venn_groups_and_pages.png}
\caption{
\textbf{(Left panel)} Overlap between the two lists of Facebook groups identified using Science Feedback data (first analysis) and Condor data (second analysis). \textbf{(Right panel)} Overlap between the three lists of Facebook pages identified using Science Feedback data (first analysis), using Condor data (second analysis) and by sharing a `reduced distribution' notification (third analysis).
}
\label{venn_accounts}
\end{figure}

We also compare the lists of pages found using the three different methods (Figure \ref{venn_accounts} right panel).
We again found a significant overlap between the page list for two first analyses, as 8 pages out of the 18 pages identified using Science Feedback data were also found using Condor data.
Interestingly the overlap between of the two first lists and the 83 pages sharing a `reduced distribution' notification was almost null (with only 2 pages in common with the first list, and 1 page with the second list).
The `reduced distribution' notifications used to identified the last set of pages were mostly shared during the last semester of 2020.
Because we kept only pages that shared 24 or more False URLs in the two first analyses, it is possible that this method was biased to select pages that rather received their `reduced distribution' notification in 2019 or earlier in 2020.
This would explain the negligible overlap between pages identified as having shared False URLs and pages identified as having shared a `reduced distribution' notification.
Obtaining such different sets of pages confirm the interest of using both methods to investigate the effects of the `repeat offender' policy.

\end{document}
